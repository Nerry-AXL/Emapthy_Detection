{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f291266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries and frameworks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import warnings\n",
    "import missingno as msno\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "from fancyimpute import IterativeImputer\n",
    "import re\n",
    "from sklearn.impute import KNNImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GroupKFold,train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6962dfaf",
   "metadata": {},
   "source": [
    "# Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Dataset\n",
    "path = r\"C:\\Users\\SHURI\\Desktop\\Final Emapthy\\EyeT\\EyeT_group_dataset_III_image_name_letter_card_participant_**_trial_*.csv\"\n",
    "filename = glob.glob(path)\n",
    "df_pre= []\n",
    "for file in filename:\n",
    "    df_pre.append(pd.read_csv(file))\n",
    "df = pd.concat(df_pre, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed059f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b9e29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dea4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Questionnaire dataset\n",
    "df_questionnaire =pd.read_csv(r\"C:\\Users\\SHURI\\Desktop\\Final Emapthy\\Questionnaire_datasetIA.csv\", encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e249cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_questionnaire.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ed4cb",
   "metadata": {},
   "source": [
    "# Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rows with missing values in \"Pupil diameter\" as pupil changes are slower than eye movements \n",
    "#and a 40 Hz frequency is sufficient for studying pupil diameter evolution over time.\n",
    "df = df.dropna(subset = ['Pupil diameter left','Pupil diameter right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a1fb76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plotting missing values for each columns\n",
    "msno.bar(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79dadf",
   "metadata": {},
   "source": [
    "Missing value plot shows that there are coloumns which are having majority of its value missing.So We are taking 75% as the threshold limits and going to remove those coloumns which are having missing values of more than 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725d2b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Function to remove columns which are having for 75% of missing values\n",
    "thres = int(0.75 * len(df))\n",
    "df = df.dropna(thresh=thres, axis=1)\n",
    "\n",
    "# Display missing value visualization\n",
    "msno.bar(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a2f310",
   "metadata": {},
   "source": [
    "The visualizations reveal the successful elimination of columns with over 75% missing values. However, it's evident that certain columns still contain gaps that require imputation.\n",
    "Furthermore, it's worth noting that certain columns within the DataFrame exhibit a consistent value across all rows. These columns, characterized by having zero variance, are not likely to contribute meaningfully to the model. As a result, we are eliminating such columns from consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ace7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing columns with the same value in every row\n",
    "unique_values = []\n",
    "for column in df.columns:\n",
    "    if df[column].nunique() <= 1:\n",
    "        unique_values.append(column)\n",
    "df = df.drop(columns=unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1921fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3744ced3",
   "metadata": {},
   "source": [
    "It's also noticeable that certain columns intended to be in integer or float data formats are currently presented as objects. A more detailed examination of these columns reveals that many of them use ',' instead of '.' as the decimal separator, hindering a straightforward transformation to floats. Thus, the ',' should be replaced with '.' for consistency.\r\n",
    "\r\n",
    "Furthermore, some columns are in an incorrect format that necessitates a type conversion. Adding to this, there's an absence of the target column, which corresponds to the empathy score, within this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa295864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing comma with dot and convert object datapoints to numeric\n",
    "df = df.replace(',', '.', regex=True)\n",
    "for col in df.columns:\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empathy scores dictionary\n",
    "empathy_scores = {}\n",
    "for index, row in df_questionnaire.iterrows():\n",
    "    empathy_scores[int(row['Participant nr'])] = row['Total Score extended']\n",
    "\n",
    "# Extracting participant numbers and converting to int\n",
    "df['Participant name'] = df['Participant name'].str[-2:].astype(int)\n",
    "\n",
    "# Sorting the DataFrame by participant name\n",
    "df = df.sort_values('Participant name')\n",
    "\n",
    "# Adding Empathy Score column to the DataFrame\n",
    "df['Empathy Score'] = df['Participant name'].apply(lambda x: empathy_scores.get(x, 0))\n",
    "\n",
    "# Sorting the DataFrame again by participant name\n",
    "df = df.sort_values('Participant name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0332ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "eyetrackertimestamp = df['Eyetracker timestamp'].value_counts(dropna=False)\n",
    "eyetrackertimestamp.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2ee4de",
   "metadata": {},
   "source": [
    "The graph indicates anomalies in data recording, where a single timestamp should ideally correspond to only one observation. However, our dataset exhibits multiple entries for certain timestamps. This discrepancy highlights the existence of duplicates that require elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a950c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates based on 'Eyetracker timestamp' column\n",
    "df = df.drop_duplicates(subset='Eyetracker timestamp')\n",
    "\n",
    "# Plotting histogram of 'Eyetracker timestamp'\n",
    "eyetracker_timestamp = df['Eyetracker timestamp'].value_counts(dropna=False)\n",
    "eyetracker_timestamp.hist()\n",
    "# The DataFrame 'df' will now contains the duplicates removed and the histogram plotted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064eb75e",
   "metadata": {},
   "source": [
    "Above Graph has shows us that we have removed all the datapoints which are having duplicate values based on eyetracker time stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb93f7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f360284f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_val_columns = ['Gaze point X (MCSnorm)', 'Gaze point Y (MCSnorm)',\n",
    "                      'Gaze point left X (MCSnorm)', 'Gaze point left Y (MCSnorm)',\n",
    "                      'Gaze point right X (MCSnorm)', 'Gaze point right Y (MCSnorm)']\n",
    "\n",
    "# Impute missing values using IterativeImputer\n",
    "imputer = IterativeImputer()\n",
    "df[missing_val_columns] = imputer.fit_transform(df[missing_val_columns])\n",
    "\n",
    "# Display missing value visualization\n",
    "msno.bar(df)\n",
    "# The DataFrame 'df' now contains the missing values imputed and the missing value visualization displayed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d760d8",
   "metadata": {},
   "source": [
    "This plot shows us that all the missing values from the dataset III has been imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b912c",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dfdb1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting the first time series\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x=df['Eyetracker timestamp'], y=df['Pupil diameter left'], data=df)\n",
    "plt.xlabel('Time Stamp')\n",
    "plt.ylabel('Pupil diameter left')\n",
    "plt.title('Time Series Plot - Pupil diameter (left) vs Time Stamp')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the second time series\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x=df['Eyetracker timestamp'], y=df['Pupil diameter right'], data=df)\n",
    "plt.xlabel('Time Stamp')\n",
    "plt.ylabel('Pupil diameter right')\n",
    "plt.title('Time Series Plot - Pupil diameter (right) vs Time Stamp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0704f1e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.hist(bins=50, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c1b0d",
   "metadata": {},
   "source": [
    "The presented plot illustrates the value distribution across each column. Notably, certain columns exhibit significant skewness in their distributions, warranting their removal.\n",
    "\n",
    "\r\n",
    "Both the plot and the referenced paper lead to the inference that Gaze event duration, Pupil diameters, Gaze points, Gaze directions, and Eye Positions are the pivotal predictor variables for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63212300",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_event_duration = {}\n",
    "for index, row in df.iterrows():\n",
    "    participant_name = row['Participant name']\n",
    "    gaze_duration = row['Gaze event duration']\n",
    "    if participant_name in gaze_event_duration:\n",
    "        gaze_event_duration[participant_name] += gaze_duration\n",
    "    else:\n",
    "        gaze_event_duration[participant_name] = gaze_duration\n",
    "\n",
    "new_df = pd.DataFrame(gaze_event_duration.items(), columns=[\"Participant name\", \"Gaze event duration\"])\n",
    "new_df['Empathy Score'] = new_df['Participant name'].apply(lambda x: empathy_scores.get(x, 0))\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=new_df['Gaze event duration'], y=new_df['Empathy Score'], data=new_df)\n",
    "plt.xlabel('Total Gaze event duration')\n",
    "plt.ylabel('Empathy Score')\n",
    "plt.title('Total Gaze event duration vs Empathy Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290db841",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_record_duration = {}\n",
    "for index, row in df.iterrows():\n",
    "    participant_name = row['Participant name']\n",
    "    record_duration = row['Gaze event duration']\n",
    "    if participant_name in total_record_duration:\n",
    "        total_record_duration[participant_name] += record_duration\n",
    "    else:\n",
    "        total_record_duration[participant_name] = record_duration\n",
    "\n",
    "new_df = pd.DataFrame(total_record_duration.items(), columns=[\"Participant name\", \"Total Record Duration\"])\n",
    "new_df['Empathy Score'] = new_df['Participant name'].apply(lambda x: empathy_scores.get(x, 0))\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=new_df['Total Record Duration'], y=new_df['Empathy Score'], data=new_df)\n",
    "plt.xlabel('Total Record Duration')\n",
    "plt.ylabel('Empathy Score')\n",
    "plt.title('Total Record Duration vs Empathy Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367dc98a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping non-numeric columns before computing the correlation matrix\n",
    "numeric_df = df.select_dtypes(include=['number'])\n",
    "\n",
    "# Computing the correlation matrix\n",
    "cor_matrix = numeric_df.drop('Empathy Score', axis=1).corr()\n",
    "\n",
    "# Plotting the correlation heatmap\n",
    "plt.subplots(figsize=(42, 42))\n",
    "plt.title('Pearson Correlation Matrix')\n",
    "sns.heatmap(cor_matrix, vmax=0.13, annot=True)\n",
    "\n",
    "# Identifying columns with high correlation\n",
    "cor_col = set()\n",
    "for i in range(len(cor_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(cor_matrix.iloc[i, j]) > 0.7:\n",
    "            col_n = cor_matrix.columns[i]\n",
    "            cor_col.add(col_n)\n",
    "\n",
    "print('Columns with Correlation are -', cor_col)\n",
    "\n",
    "# Dropping correlating columns\n",
    "df = df.drop(columns=cor_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88ec9c-70f9-4923-ac69-7cd37c4d007c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
