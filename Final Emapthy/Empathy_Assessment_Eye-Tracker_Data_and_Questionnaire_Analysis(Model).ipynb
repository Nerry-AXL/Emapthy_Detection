{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee543a61-f245-4506-b56a-ac7a58346bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries and frameworks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import warnings\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GroupKFold,train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ed174-7799-4053-912e-27742536b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the participant's number from the file's name\n",
    "def participant_number_extraction(string):\n",
    "    #Regular expression pattern for matching the format 'participant_x'\n",
    "    pattern = r'participant_(\\d+)'\n",
    "    # Matching the pattern with the filename\n",
    "    match = re.search(pattern, string)\n",
    "    # Extracting the participant number and convert it to an integer when match is found\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    # If no match was found, return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a304b-721f-4e86-86cb-fa1ab08ece0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_non_null_row(df, col_name):\n",
    "    # Getting a Series of boolean values indicating whether each row has a non-null value in the specified column\n",
    "    non_null_mask = df[col_name].notnull()\n",
    "    # Finding the index of the first True value in the Series \n",
    "    first_non_null_idx = non_null_mask.idxmax()\n",
    "    return first_non_null_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843597dd-5d21-4811-a5c3-85349611146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(df):\n",
    "    col = 'Pupil diameter left'\n",
    "    #Finding the 1st row with non null value on pupil diameter left column\n",
    "    starting_index_left = get_first_non_null_row(df,col)\n",
    "    col = 'Pupil diameter right'\n",
    "    #Finding the 1st row with non null value on pupil diameter right column\n",
    "    starting_index_right = get_first_non_null_row(df,col)\n",
    "    #Comparing both to find the 1st measured value of pupil diameter\n",
    "    if starting_index_left < starting_index_right :\n",
    "        selected_df = df.iloc[starting_index_left::3] # Incrementing 3 rows each time as pupil diameter is measered on 40HQs frequency\n",
    "    else :\n",
    "        selected_df = df.iloc[starting_index_right::3]\n",
    "    return selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb136a2-66c6-457e-9569-6c74b1fc7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_val_cols(df):\n",
    "    unique_values = []\n",
    "    for column in df.columns :\n",
    "        if column != 'Participant name' and column != 'Recording timestamp':\n",
    "# If columns have unique values more than 1, it means that columns have variance\n",
    "            if df[column].nunique() <= 1:\n",
    "                unique_values.append(column)\n",
    "    df = df.drop(columns = unique_values) # Removing columns with zero variance\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d40c50-0140-4ba0-a034-526ccf2ca8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for removal of columns that do not affect the target and hence will not be used in course of the model training\n",
    "def unwanted_columns_removal(df, unwanted_cols):\n",
    "    df = df.drop(columns = unwanted_cols, axis =1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efdbe39-e025-4183-952f-a360f6d8e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for type casting object data points into numerical data points\n",
    "def numeric_conversion(df):\n",
    "    df = df.replace(',','.',regex = True)\n",
    "    for col in df.columns:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706dca5b-6b36-4e2e-b288-6ec7618367ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove duplicates from the dataset considering the Eyetracker Time Stamp Column\n",
    "def duplicate_removal(df):\n",
    "    df = df.drop_duplicates(subset = 'Recording timestamp')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315fafa-48b5-41fc-807e-b25e9c488d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for mapping empathy scores which is our target variable to participant number\n",
    "def empathy_score(df,df_question):\n",
    "    empathy_scores= {} # Empathy score dictonary\n",
    "    for index, row in df_question.iterrows():\n",
    "        empathy_scores[int(row['Participant nr'])] = row['Total Score extended'] # Adding empathy to the dictornary as value and participant number as the key\n",
    "    df['Empathy Score'] = 0\n",
    "    df['Empathy Score'] = df['Participant name'].apply(lambda x: empathy_scores.get(x, 0)) # Adding Empathy Score to the dataframe based on the participant number\n",
    "    return df, empathy_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c16e2a-d5d4-4fea-9e5a-7173aa10ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion for data imputation using the K-Nearest Neighbors (KNN) imputation technique\n",
    "def imputation(df):\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    imputed_df = imputer.fit_transform(df)\n",
    "    imputed_df = pd.DataFrame(imputed_df, columns=df.columns)\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf42397-3cb4-426a-9626-66b97ffccf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for processing a DataFrame by selecting specific columns, calculating mode values for those columns, \n",
    "# and then creating a summary DataFrame containing these mode values and additional \n",
    "def moddata(df):\n",
    "    selected_columns = [\n",
    "        'Gaze point X', 'Gaze point Y',\n",
    "        'Pupil diameter left', 'Pupil diameter right',\n",
    "        'Eye position left X (DACSmm)', 'Eye position left Y (DACSmm)',\n",
    "        'Eye position left Z (DACSmm)', 'Eye position right X (DACSmm)',\n",
    "        'Eye position right Y (DACSmm)', 'Eye position right Z (DACSmm)',\n",
    "        'Gaze event duration', 'Empathy Score'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        mode_values = df[selected_columns].mode().iloc[0]\n",
    "    except KeyError as e:\n",
    "        missing_col = str(e).strip('\\'[]\\'')\n",
    "        print(f\"Column {missing_col} not found in the dataframe\")\n",
    "        return None\n",
    "    \n",
    "    mode_df = pd.DataFrame([mode_values], columns=selected_columns)\n",
    "    mode_df['Participant name'] = df['Participant name'].iloc[0]\n",
    "    mode_df['Avg Gaze event duration'] = df['Gaze event duration'].mean()\n",
    "    mode_df['Total Gaze event duration'] = df['Gaze event duration'].sum()\n",
    "    \n",
    "    return mode_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043795d0-dfe6-4988-b122-a154fa0f9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_correlation(df):\n",
    "    cor_matrix = df.corr()\n",
    "    plt.subplots(figsize = (42,42))\n",
    "    plt.title('Pearson Corealation Matrix')\n",
    "    sns.heatmap(cor_matrix,vmax = 0.13,annot=True)\n",
    "    cor_col = set()\n",
    "    for i in range(len(cor_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(cor_matrix.iloc[i,j]) > .7 :\n",
    "                col_n = cor_matrix.columns[i]\n",
    "                cor_col.add(col_n)\n",
    "    print('Columns with Corelation are - ', cor_col)\n",
    "    df  = df.drop(columns = cor_col, axis =1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65b88e-c04c-4ca1-a01a-d1c87547e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df)\n",
    "    X_scaled = pd.DataFrame(scaler.transform(df),columns=df.columns)\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b8ecb3-8450-445c-aca8-3c20f2619015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_seperation(df):\n",
    "    y_df = df['Empathy Score']\n",
    "    x_df = df.drop(['Empathy Score'], axis= 1)\n",
    "    return x_df,y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53f24b-ad80-45b1-bc8b-3a5ce751e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(x_df, y_df, var):\n",
    "    groups = x_df['Participant name'].tolist()\n",
    "    n_splits = 10\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    model = RandomForestRegressor(n_estimators=10, random_state=13)\n",
    "    scores = []\n",
    "\n",
    "    for train_index, test_index in gkf.split(x_df, y_df, groups=groups):\n",
    "        x_train, x_test = x_df.drop(['Participant name'], axis=1).iloc[train_index], x_df.drop(['Participant name'], axis=1).iloc[test_index]\n",
    "        y_train, y_test = y_df.iloc[train_index], y_df.iloc[test_index]\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = 1 - (mse / var)\n",
    "        scores.append(r2)\n",
    "        print(\"R2 score - Cross: {:.3f}\".format(r2))\n",
    "        print(\"MSE - Cross: {:.3f}\".format(mse))\n",
    "        print(\"Var: {:.3f}\".format(var))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(1, n_splits + 1), scores, marker='*')\n",
    "    ax.set_xlabel('Sample')\n",
    "    ax.set_ylabel('R-Squared')\n",
    "    ax.set_title('Cross-Validation Scores')\n",
    "    plt.show()\n",
    "\n",
    "# Visualizing the results from the cross-validation\n",
    "cross_validation(x_df, y_df, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a8809-5b53-43e0-b804-8ffa13dd7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x_df, y_df, test_size, random_state):\n",
    "    groups = x_df['Participant name'].tolist()\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    train_idx, test_idx = next(gss.split(x_df, y_df, groups=groups))\n",
    "    \n",
    "    x_train = x_df.drop('Participant name', axis=1).iloc[train_idx]\n",
    "    x_test = x_df.drop('Participant name', axis=1).iloc[test_idx]\n",
    "    \n",
    "    y_train = y_df.iloc[train_idx]\n",
    "    y_test = y_df.iloc[test_idx]\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "# Usage\n",
    "test_size = 0.2\n",
    "random_state = 45\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size, random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b28a1a7-82df-4f2a-9ca9-bd9260ec8424",
   "metadata": {},
   "source": [
    "def model_training (x_train, x_test, y_train, y_test, var):\n",
    "    model = RandomForestRegressor(n_estimators=10, random_state=13)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = 1 - (mse / var)\n",
    "    \n",
    "    print(\"R2 score: {:.3f}\".format(r2))\n",
    "    print(\"MSE: {:.3f}\".format(mse))\n",
    "    \n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    print(\"Features in order of priority:\")\n",
    "    for f in range(x_train.shape[1]):\n",
    "        print(\"%d. feature %s (%f)\" % (f + 1, x_train.columns[indices[f]], importances[indices[f]]))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(x_train.shape[1]), importances[indices], color=\"r\", align=\"center\")\n",
    "    plt.xticks(range(x_train.shape[1]), x_train.columns[indices], rotation='vertical')\n",
    "    plt.xlim([-1, x_train.shape[1]])\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "model_training(x_train, x_test, y_train, y_test, var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748d09c-1aa2-44c4-b4ac-e9bf866ebd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a neural network model and evaluate its performance\n",
    "def model_training(x_train, x_test, y_train, y_test, var):\n",
    "    # Create a sequential neural network model\n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(Dense(64, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "    nn_model.add(Dense(32, activation='relu'))\n",
    "    nn_model.add(Dense(1))\n",
    "\n",
    "    # Compile the model with the Adam optimizer and mean squared error loss\n",
    "    nn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    # Train the neural network model using training data\n",
    "    nn_model.fit(x_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    y_pred_nn = nn_model.predict(x_test)\n",
    "\n",
    "    # Create a deep neural network model\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "    dnn_model.add(Dense(64, activation='relu'))\n",
    "    dnn_model.add(Dense(32, activation='relu'))\n",
    "    dnn_model.add(Dense(1))\n",
    "\n",
    "    # Compile the model with the Adam optimizer and mean squared error loss\n",
    "    dnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    # Train the deep neural network model using training data\n",
    "    dnn_model.fit(x_train, y_train, epochs=150, batch_size=64, verbose=0)\n",
    "\n",
    "    # Make predictions using the trained DNN model\n",
    "    y_pred_dnn = dnn_model.predict(x_test)\n",
    "\n",
    "    # Calculate mean squared error and R-squared score for NN and DNN\n",
    "    mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "    r2_nn = 1 - (mse_nn / var)\n",
    "\n",
    "    mse_dnn = mean_squared_error(y_test, y_pred_dnn)\n",
    "    r2_dnn = 1 - (mse_dnn / var)\n",
    "\n",
    "    # Print evaluation metrics for both NN and DNN\n",
    "    print(\"Neural Network R2 score: {:.3f}\".format(r2_nn))\n",
    "    print(\"Neural Network MSE: {:.3f}\".format(mse_nn))\n",
    "\n",
    "    print(\"Deep Neural Network R2 score: {:.3f}\".format(r2_dnn))\n",
    "    print(\"Deep Neural Network MSE: {:.3f}\".format(mse_dnn))\n",
    "\n",
    "    # Visualize the predictions against the actual values for NN and DNN\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_test, y_pred_nn, alpha=0.5)\n",
    "    plt.title(\"Neural Network: Actual vs. Predicted Empathy Scores\")\n",
    "    plt.xlabel(\"Actual Empathy Scores\")\n",
    "    plt.ylabel(\"Predicted Empathy Scores\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_test, y_pred_dnn, alpha=0.5)\n",
    "    plt.title(\"Deep Neural Network: Actual vs. Predicted Empathy Scores\")\n",
    "    plt.xlabel(\"Actual Empathy Scores\")\n",
    "    plt.ylabel(\"Predicted Empathy Scores\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "model_training(x_train, x_test, y_train, y_test, var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954e7df-d745-49ca-b7e7-6145d1f67b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def model_training(x_train, x_test, y_train, y_test, var):\n",
    "    # SGDRegressor\n",
    "    sgd_model = SGDRegressor(random_state=13)\n",
    "    sgd_model.fit(x_train, y_train)\n",
    "    y_pred_sgd = sgd_model.predict(x_test)\n",
    "    mse_sgd = mean_squared_error(y_test, y_pred_sgd)\n",
    "    r2_sgd = 1 - (mse_sgd / var)\n",
    "    print(\"SGDRegressor:\")\n",
    "    print(\"R2 score: {:.3f}\".format(r2_sgd))\n",
    "    print(\"MSE: {:.3f}\".format(mse_sgd))\n",
    "\n",
    "    # Linear Regression\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    y_pred_lr = lr_model.predict(x_test)\n",
    "    mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "    r2_lr = 1 - (mse_lr / var)\n",
    "    print(\"\\nLinear Regression:\")\n",
    "    print(\"R2 score: {:.3f}\".format(r2_lr))\n",
    "    print(\"MSE: {:.3f}\".format(mse_lr))\n",
    "\n",
    "    # XGBoost Regressor\n",
    "    xgb_model = XGBRegressor(random_state=13)\n",
    "    xgb_model.fit(x_train, y_train)\n",
    "    y_pred_xgb = xgb_model.predict(x_test)\n",
    "    mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "    r2_xgb = 1 - (mse_xgb / var)\n",
    "    print(\"\\nXGBoost Regressor:\")\n",
    "    print(\"R2 score: {:.3f}\".format(r2_xgb))\n",
    "    print(\"MSE: {:.3f}\".format(mse_xgb))\n",
    "\n",
    "    # Neural Networks (Deep Neural Networks)\n",
    "    nn_model = keras.Sequential([\n",
    "        keras.layers.Dense(64, activation='relu', input_shape=(x_train.shape[1],)),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    nn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    nn_model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    y_pred_nn = nn_model.predict(x_test).flatten()\n",
    "    mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "    r2_nn = 1 - (mse_nn / var)\n",
    "    print(\"\\nNeural Networks (Deep Neural Networks):\")\n",
    "    print(\"R2 score: {:.3f}\".format(r2_nn))\n",
    "    print(\"MSE: {:.3f}\".format(mse_nn))\n",
    "\n",
    "    # Feature importances (for SGDRegressor)\n",
    "    if hasattr(sgd_model, 'coef_'):\n",
    "        importances = np.abs(sgd_model.coef_)\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        print(\"\\nFeatures in order of priority (SGDRegressor):\")\n",
    "        for f in range(x_train.shape[1]):\n",
    "            print(\"%d. feature %s (%f)\" % (f + 1, x_train.columns[indices[f]], importances[indices[f]]))\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.title(\"Feature importances (SGDRegressor)\")\n",
    "        plt.bar(range(x_train.shape[1]), importances[indices], color=\"r\", align=\"center\")\n",
    "        plt.xticks(range(x_train.shape[1]), x_train.columns[indices], rotation='vertical')\n",
    "        plt.xlim([-1, x_train.shape[1]])\n",
    "        plt.show()\n",
    "\n",
    "# Usage\n",
    "model_training(x_train, x_test, y_train, y_test, var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f1662-aab6-48e0-9084-7f893dd5df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def model_training(x_train, x_test, y_train, y_test, var):\n",
    "    # SGDRegressor\n",
    "    sgd_model = SGDRegressor(random_state=13)\n",
    "    sgd_model.fit(x_train, y_train)\n",
    "    y_pred_sgd = sgd_model.predict(x_test)\n",
    "    mse_sgd = mean_squared_error(y_test, y_pred_sgd)\n",
    "    r2_sgd = 1 - (mse_sgd / var)\n",
    "    print(\"SGDRegressor:\")\n",
    "    print(\"R2 score: {:.3f}\".format(r2_sgd))\n",
    "    print(\"MSE: {:.3f}\".format(mse_sgd))\n",
    "\n",
    "    # Linear Regression\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    y_pred_lr = lr_model.predict(x_test)\n",
    "    mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "    r2_lr = 1 - (mse_lr / var)\n",
    "    print(\"\\nLinear Regression:\")\n",
    "    print(\"R2 score: {:.3f}\".format(r2_lr))\n",
    "    print(\"MSE: {:.3f}\".format(mse_lr))\n",
    "\n",
    "    # XGBoost Regressor\n",
    "    xgb_model = XGBRegressor(random_state=13)\n",
    "    xgb_model.fit(x_train, y_train)\n",
    "    y_pred_xgb = xgb_model.predict(x_test)\n",
    "    mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "    r2_xgb = 1 - (mse_xgb / var)\n",
    "    print(\"\\nXGBoost Regressor:\")\n",
    "    print(\"R2 score: {:.3f}\".format(r2_xgb))\n",
    "    print(\"MSE: {:.3f}\".format(mse_xgb))\n",
    "\n",
    "    # Neural Networks (Deep Neural Networks)\n",
    "    nn_model = keras.Sequential([\n",
    "        keras.layers.Dense(64, activation='relu', input_shape=(x_train.shape[1],)),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    nn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    nn_model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    y_pred_nn = nn_model.predict(x_test).flatten()\n",
    "    mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "    r2_nn = 1 - (mse_nn / var)\n",
    "    print(\"\\nNeural Networks (Deep Neural Networks):\")\n",
    "    print(\"R2 score: {:.3f}\".format(r2_nn))\n",
    "    print(\"MSE: {:.3f}\".format(mse_nn))\n",
    "\n",
    "    # Feature importances (for SGDRegressor)\n",
    "    if hasattr(sgd_model, 'coef_'):\n",
    "        importances = np.abs(sgd_model.coef_)\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        print(\"\\nFeatures in order of priority (SGDRegressor):\")\n",
    "        for f in range(x_train.shape[1]):\n",
    "            print(\"%d. feature %s (%f)\" % (f + 1, x_train.columns[indices[f]], importances[indices[f]]))\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.title(\"Feature importances (SGDRegressor)\")\n",
    "        plt.bar(range(x_train.shape[1]), importances[indices], color=\"r\", align=\"center\")\n",
    "        plt.xticks(range(x_train.shape[1]), x_train.columns[indices], rotation='vertical')\n",
    "        plt.xlim([-1, x_train.shape[1]])\n",
    "        plt.show()\n",
    "\n",
    "# Usage\n",
    "model_training(x_train, x_test, y_train, y_test, var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8096e2-0cd2-4b49-9ee7-da790cf3e249",
   "metadata": {},
   "source": [
    "##READING THE DATA AND SUBJECTING IT TO THE FUNTIONS FOR TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0cfc47-c4e4-4abc-b484-4b77e59c75e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\SHURI\\Desktop\\Final Emapthy\\EyeT\\EyeT_group_dataset_III_image_name_letter_card_participant_**_trial_*.csv\"\n",
    "df_question =pd.read_csv(r\"C:\\Users\\SHURI\\Desktop\\Final Emapthy\\Questionnaire_datasetIB.csv\", encoding = 'ISO-8859-1')\n",
    "filename = glob.glob(path)\n",
    "df = pd.DataFrame()\n",
    "for file in filename:\n",
    "    df_pre = pd.read_csv(file)\n",
    "    participant_number = extract_participant_number(file)\n",
    "    df_pre['Participant name'] = participant_number\n",
    "    selected_df = feature_selection(df_pre)\n",
    "    selected_df = numeric_conversion(selected_df)\n",
    "    selected_df = duplicate_removal(selected_df)\n",
    "    unwanted_cols = ['Eye movement type','Computer timestamp','Eye movement type index','Unnamed: 0','Sensor',\n",
    "                     'Event','Event value','Validity left','Validity right','Presented Stimulus name',\n",
    "                     'Presented Media width','Presented Media name','Presented Media height','Presented Media position X (DACSpx)',\n",
    "                     'Presented Media position Y (DACSpx)','Original Media width','Original Media height','Mouse position X','Mouse position Y']\n",
    "    selected_df = unwanted_columns_removal(selected_df, unwanted_cols)\n",
    "    selected_df = unique_val_cols(selected_df)\n",
    "    selected_df,empathy_scores = empathy_score(selected_df,df_question)\n",
    "    selected_df = imputation(selected_df)\n",
    "    selected_df = moddata(selected_df)\n",
    "    df = pd.concat([df,selected_df])\n",
    "\n",
    "df = min_max_scaler(df)\n",
    "x_df, y_df = feature_seperation(df)\n",
    "var = np.var(y_df)\n",
    "x_df = drop_correlation(x_df)\n",
    "cross_validation(x_df,y_df,var)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=85)\n",
    "model = model_training(x_train, x_test, y_train, y_test,var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08880714-c570-4f59-92a9-0e922d442931",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\SHURI\\Desktop\\Final Emapthy\\EyeT\\EyeT_group_dataset_II_image_name_grey_blue_participant_**_trial_*.csv\"\n",
    "df_question =pd.read_csv(r\"C:\\Users\\SHURI\\Desktop\\Final Emapthy\\Questionnaire_datasetIB.csv\", encoding = 'ISO-8859-1')\n",
    "filename = glob.glob(path)\n",
    "df = pd.DataFrame()\n",
    "for file in filename:\n",
    "    df_pre = pd.read_csv(file)\n",
    "    participant_number = participant_number_extraction(file)\n",
    "    df_pre['Participant name'] = participant_number\n",
    "    selected_df = feature_selection(df_pre)\n",
    "    selected_df = numeric_conversion(selected_df)\n",
    "    selected_df = duplicate_removal(selected_df)\n",
    "    unwanted_cols = ['Eye movement type','Computer timestamp','Eye movement type index','Unnamed: 0','Sensor',\n",
    "                     'Event','Event value','Validity left','Validity right','Presented Stimulus name','Presented Media width',\n",
    "                     'Presented Media name','Presented Media height','Presented Media position X (DACSpx)',\n",
    "                     'Presented Media position Y (DACSpx)','Original Media width','Original Media height']\n",
    "    selected_df = unwanted_columns_removal(selected_df, unwanted_cols)\n",
    "    selected_df = unique_val_cols(selected_df)\n",
    "    selected_df,empathy_scores = empathy_score(selected_df,df_question)\n",
    "    selected_df = imputation(selected_df)\n",
    "    selected_df = moddata(selected_df)\n",
    "    df = pd.concat([df,selected_df])\n",
    "    \n",
    "df = min_max_scaler(df)\n",
    "x_df, y_df = feature_seperation(df)\n",
    "var = variance = np.var(y_df)\n",
    "x_df = drop_correlation(x_df)\n",
    "cross_validation(x_df,y_df,var)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=45)\n",
    "model_training(x_train, x_test, y_train, y_test,var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf0b57-16dc-4342-8dd5-42641bd10d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
